<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
<configuration>
  <property>
    <name>zeppelin_env_content</name>
    <description>This is the jinja template for zeppelin-env.sh file</description>
    <value>
# Spark master url. eg. spark://master_addr:7077. Leave empty if you want to use local mode
export MASTER=yarn-client
export SPARK_YARN_JAR={{spark_jar}}


# Where log files are stored.  PWD by default.
export ZEPPELIN_LOG_DIR={{zeppelin_log_dir}}

# The pid files are stored. /tmp by default.
export ZEPPELIN_PID_DIR={{zeppelin_pid_dir}}


export JAVA_HOME={{java64_home}}

# Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS="-Dspark.executor.memory=8g -Dspark.cores.max=16"
export ZEPPELIN_JAVA_OPTS="-Dhdp.version={{full_stack_version}} -Dspark.executor.memory={{executor_mem}} -Dspark.executor.instances={{executor_instances}} -Dspark.yarn.queue={{spark_queue}}"


# Zeppelin jvm mem options Default -Xmx1024m -XX:MaxPermSize=512m
# export ZEPPELIN_MEM

# zeppelin interpreter process jvm mem options. Defualt = ZEPPELIN_MEM
# export ZEPPELIN_INTP_MEM

# zeppelin interpreter process jvm options. Default = ZEPPELIN_JAVA_OPTS
# export ZEPPELIN_INTP_JAVA_OPTS

# Where notebook saved
# export ZEPPELIN_NOTEBOOK_DIR

# Id of notebook to be displayed in homescreen. ex) 2A94M5J1Z
# export ZEPPELIN_NOTEBOOK_HOMESCREEN

# hide homescreen notebook from list when this value set to "true". default "false"
# export ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE

# Bucket where notebook saved
# export ZEPPELIN_NOTEBOOK_S3_BUCKET

# User in bucket where notebook saved. For example bucket/user/notebook/2A94M5J1Z/note.json
# export ZEPPELIN_NOTEBOOK_S3_USER

# A string representing this instance of zeppelin. $USER by default
# export ZEPPELIN_IDENT_STRING

# The scheduling priority for daemons. Defaults to 0.
# export ZEPPELIN_NICENESS


#### Spark interpreter configuration ####

## Use provided spark installation ##
## defining SPARK_HOME makes Zeppelin run spark interpreter process using spark-submit
##
# (required) When it is defined, load it instead of Zeppelin embedded Spark libraries
export SPARK_HOME={{spark_home}}

# (optional) extra options to pass to spark submit. eg) "--driver-memory 512M --executor-memory 1G".
# export SPARK_SUBMIT_OPTIONS

## Use embedded spark binaries ##
## without SPARK_HOME defined, Zeppelin still able to run spark interpreter process using embedded spark binaries.
## however, it is not encouraged when you can define SPARK_HOME
##
# Options read in YARN client mode
# yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.
export HADOOP_CONF_DIR=/etc/hadoop/conf

# Pyspark (supported with Spark 1.2.1 and above)
# To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI
# path to the python command. must be the same path on the driver(Zeppelin) and all workers.
# export PYSPARK_PYTHON

export PYTHONPATH="${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip"
export SPARK_YARN_USER_ENV="PYTHONPATH=${PYTHONPATH}"

## Spark interpreter options ##
##
# Use HiveContext instead of SQLContext if set true. true by default.
# export ZEPPELIN_SPARK_USEHIVECONTEXT

# Execute multiple SQL concurrently if set true. false by default.
# export ZEPPELIN_SPARK_CONCURRENTSQL

# Max number of SparkSQL result to display. 1000 by default.
# export ZEPPELIN_SPARK_MAXRESULT

  </value>
    <on-ambari-upgrade add="true"/>
  </property>
</configuration>
